'''Deespeech utilities, used for splitting wavs via VAD'''

import scipy.io.wavfile as wav
import multiprocessing.dummy as mp
import shutil, os, pathlib, pickle, sys, math, importlib, json.tool
import numpy as np
from os.path import join, exists
from tqdm import tqdm
from itertools import product
import glob
import webrtcvad
from timeit import default_timer as timer
import collections
import contextlib
import wave
import librosa
import soundfile as sf

def read_wave(path):
    """Reads a .wav file.
    Takes the path, and returns (PCM audio data, sample rate).
    """
    with contextlib.closing(wave.open(path, 'rb')) as wf:
        num_channels = wf.getnchannels()
        assert num_channels == 1
        sample_width = wf.getsampwidth()
        assert sample_width == 2, 'sample width is '+str(sample_width) + path
        sample_rate = wf.getframerate()
        if sample_rate != 16000:
            print('Rewriting b/c sample rate is', sample_rate)
            x, sr = librosa.load(path)
            target_sr = 16000
            x = librosa.resample(x,orig_sr=sr,target_sr=target_sr)
            sf.write(path, x, target_sr)
            return read_wave(path)

        assert sample_rate in (8000, 16000, 32000), f'sample_rate is {sample_rate} which is not supported'
        frames = wf.getnframes()
        pcm_data = wf.readframes(frames)
        duration = frames / sample_rate
        return pcm_data, sample_rate, duration


def write_wave(path, audio, sample_rate):
    """Writes a .wav file.
    Takes path, PCM audio data, and sample rate.
    """
    with contextlib.closing(wave.open(path, 'wb')) as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio)


class Frame(object):
    """Represents a "frame" of audio data."""
    def __init__(self, bytes, timestamp, duration):
        self.bytes = bytes
        self.timestamp = timestamp
        self.duration = duration


def frame_generator(frame_duration_ms, audio, sample_rate):
    """Generates audio frames from PCM audio data.
    Takes the desired frame duration in milliseconds, the PCM data, and
    the sample rate.
    Yields Frames of the requested duration.
    """
    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)
    offset = 0
    timestamp = 0.0
    duration = (float(n) / sample_rate) / 2.0
    while offset + n < len(audio):
        yield Frame(audio[offset:offset + n], timestamp, duration)
        timestamp += duration
        offset += n


def vad_collector(sample_rate, frame_duration_ms,
                  padding_duration_ms, vad, frames):
    """Filters out non-voiced audio frames.
    Given a webrtcvad.Vad and a source of audio frames, yields only
    the voiced audio.
    Uses a padded, sliding window algorithm over the audio frames.
    When more than 90% of the frames in the window are voiced (as
    reported by the VAD), the collector triggers and begins yielding
    audio frames. Then the collector waits until 90% of the frames in
    the window are unvoiced to detrigger.
    The window is padded at the front and back to provide a small
    amount of silence or the beginnings/endings of speech around the
    voiced frames.
    Arguments:
    sample_rate - The audio sample rate, in Hz.
    frame_duration_ms - The frame duration in milliseconds.
    padding_duration_ms - The amount to pad the window, in milliseconds.
    vad - An instance of webrtcvad.Vad.
    frames - a source of audio frames (sequence or generator).
    Returns: A generator that yields PCM audio data.
    """
    num_padding_frames = int(padding_duration_ms / frame_duration_ms)
    # We use a deque for our sliding window/ring buffer.
    ring_buffer = collections.deque(maxlen=num_padding_frames)
    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the
    # NOTTRIGGERED state.
    triggered = False

    voiced_frames = []
    for frame in frames:
        is_speech = vad.is_speech(frame.bytes, sample_rate)

        if not triggered:
            ring_buffer.append((frame, is_speech))
            num_voiced = len([f for f, speech in ring_buffer if speech])
            # If we're NOTTRIGGERED and more than 90% of the frames in
            # the ring buffer are voiced frames, then enter the
            # TRIGGERED state.
            if num_voiced > 0.9 * ring_buffer.maxlen:
                triggered = True
                # We want to yield all the audio we see from now until
                # we are NOTTRIGGERED, but we have to start with the
                # audio that's already in the ring buffer.
                for f, s in ring_buffer:
                    voiced_frames.append(f)
                ring_buffer.clear()
        else:
            # We're in the TRIGGERED state, so collect the audio data
            # and add it to the ring buffer.
            voiced_frames.append(frame)
            ring_buffer.append((frame, is_speech))
            num_unvoiced = len([f for f, speech in ring_buffer if not speech])
            # If more than 90% of the frames in the ring buffer are
            # unvoiced, then enter NOTTRIGGERED and yield whatever
            # audio we've collected.
            if num_unvoiced > 0.9 * ring_buffer.maxlen:
                triggered = False
                yield b''.join([f.bytes for f in voiced_frames])
                ring_buffer.clear()
                voiced_frames = []
    if triggered:
        pass
    # If we have any leftover voiced audio when we run out of input,
    # yield it.
    if voiced_frames:
        yield b''.join([f.bytes for f in voiced_frames])

'''
Generate VAD segments. Filters out non-voiced audio frames.
@param waveFile: Input wav file to run VAD on.0
@Retval:
Returns tuple of
    segments: a bytearray of multiple smaller audio frames
              (The longer audio split into mutiple smaller one's)
    sample_rate: Sample rate of the input audio file
    audio_length: Duraton of the input audio file
'''
def vad_segment_generator(wavFile, aggressiveness):
    audio, sample_rate, audio_length = read_wave(wavFile)
    assert sample_rate == 16000, "Only 16000Hz input WAV files are supported for now!"
    vad = webrtcvad.Vad(int(aggressiveness))
    frames = frame_generator(30, audio, sample_rate)
    frames = list(frames)
    segments = vad_collector(sample_rate, 30, 300, vad, frames)

    return segments, sample_rate, audio_length


def obj_to_grid(a):
    '''get all objects corresponding to hyperparamter grid search
    a = {
        'hey': [1,2],
        'there': [3,4],
        'people': 5
    }
    ->
    {'hey': 1, 'there': 3, 'people': 5}
    {'hey': 1, 'there': 4, 'people': 5}
    {'hey': 2, 'there': 3, 'people': 5}
    {'hey': 2, 'there': 4, 'people': 5}
    '''

    for k,v in list(a.items()):
        if type(v) != list:
            a[k] = [v]

    to_ret = []
    for values in list(product(*list(a.values()))):
        to_ret.append({k:v for k,v in zip(a.keys(), values)})
    return to_ret

def ar(a):
    return np.array(a)

def rmtree(dir_path):
    # print(f'Removing {dir_path}')
    if os.path.isdir(dir_path):
        shutil.rmtree(dir_path)
    # else:
        # print(f'{dir_path} is not a directory, so cannot remove')

def npr(x, decimals=2):
    return np.round(x, decimals=decimals)
    
def int_to_str(*keys):
    return [list(map(lambda elt: str(elt), key)) for key in keys]
    
def rm_mkdirp(dir_path, overwrite, quiet=False):
    if os.path.isdir(dir_path):
        if overwrite:
            # if not quiet:
                # print('Removing ' + dir_path)
            shutil.rmtree(dir_path, ignore_errors=True)

        else:
            print('Directory ' + dir_path + ' exists and overwrite flag not set to true.  Exiting.')
            exit(1)
    if not quiet:
        print('Creating ' + dir_path)
    pathlib.Path(dir_path).mkdir(parents=True)

def lists_to_2d_arr(list_in, max_len=None):
    '''2d list in, but where sub lists may have differing lengths, one big padded 2d arr out'''
    max_len = max([len(elt) for elt in list_in]) if max_len is None else max_len
    new_arr = np.zeros((len(list_in), max_len))
    for i,elt in enumerate(list_in):
        if len(elt) < max_len:
            new_arr[i,:len(elt)] = elt
        else:
            new_arr[i,:] = elt[:max_len]
    return new_arr


def mkdirp(dir_path):
    if not os.path.isdir(dir_path):
        pathlib.Path(dir_path).mkdir(parents=True)

def rm_file(file_path):
    if os.path.exists(file_path):
        os.remove(file_path)
    
def rglob(dir_path, pattern):
    return list(map(lambda elt: str(elt), pathlib.Path(dir_path).rglob(pattern)))

def move_matching_files(dir_path, pattern, new_dir, overwrite):
    rm_mkdirp(new_dir, True, overwrite)
    for elt in rglob(dir_path, pattern):
        shutil.move(elt, new_dir)
    
def subset(a, b):
    return np.min([elt in b for elt in a]) > 0

def list_gpus():
    return tf.config.experimental.list_physical_devices('GPU')


def save_pk(file_stub, pk, protocol=None):
    filename = file_stub if '.pk' in file_stub else f'{file_stub}.pk'
    with open(filename, 'wb') as f:
        pickle.dump(pk, f, protocol=protocol)
    
def load_pk(file_stub):
    filename = file_stub
    if not os.path.exists(filename):
        return {}
    
    with open(filename, 'rb') as f:
        obj = pickle.load(f)
        return obj

def load_pk_old(filename):
    with open(filename, 'rb') as f:
        u = pickle._Unpickler(f)
        u.encoding = 'latin1'
        p = u.load()
        return p

def get_ints(*keys):
    return [int(key) for key in keys]

class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return json.JSONEncoder.default(self, obj)

def save_json(file_stub, obj):
    filename = file_stub
    with open(filename, 'w') as f:
        json.dump(obj, f, cls=NumpyEncoder, indent=4)

def load_json(file_stub):
    filename = file_stub
    with open(filename) as json_file:
        return json.load(json_file)

def lfilter(fn, iterable):
    return list(filter(fn, iterable))

def lkeys(obj):
    return list(obj.keys())

def lvals(obj):
    return list(obj.values())

def lmap(fn, iterable):
    return list(map(fn, iterable))

def sort_dict(d, reverse=False):
    return {k: v for k,v in sorted(d.items(), key=lambda elt: elt[1], reverse=reverse)}

def csv_path(sym):
    return join('csvs', f'{sym}.csv')

def is_unique(a):
    return len(np.unique(a)) == len(a)

def lists_equal(a,b):
    return np.all([elt in b for elt in a]) and np.all([elt in a for elt in b])
    
def split_arr(cond, arr):
    return lfilter(cond, arr), lfilter(lambda elt: not cond(elt), arr)

def lzip(*keys):
    return list(zip(*keys))

def arzip(*keys):
    return [ar(elt) for elt in lzip(*keys)]
    
def dilation_pad(max_len, max_dilation_rate):
    to_ret = math.ceil(max_len/max_dilation_rate)*max_dilation_rate
    assert (to_ret % max_dilation_rate) == 0
    return to_ret

def zero_pad_to_length(data, length):
    padAm = length - data.shape[0]
    if padAm == 0:
        return data
    else:
        return np.pad(data, ((0,padAm), (0,0)), 'constant')

def paths_to_mfbs(paths, max_len):
    '''Get normalized & padded mfbs from paths'''
    # normalize
    mfbs = None
    for file_name in paths:
        if mfbs is None:
            mfbs = np.array(np.load(file_name))
        else:
            mfbs = np.concatenate([mfbs, np.load(file_name)], axis=0)
    mean_vec = np.mean(mfbs, axis=0)
    std_vec  = np.std(mfbs, axis=0)

    # concat & pad
    mfbs = None
    for file_name in paths:
        mfb = (np.load(file_name) - mean_vec) / (std_vec + np.ones_like(std_vec)*1e-3)
        if mfbs is None:
            mfbs = np.array([zero_pad_to_length(mfb, max_len)])
        else:
            mfbs = np.concatenate([mfbs, [zero_pad_to_length(mfb, max_len)]], axis=0)
    return tf.cast(mfbs, tf.float64)

def str2bool(v):
    if isinstance(v, bool):
       return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')

def destring(y, width=3):
    ''' y is an array with elements in the format '.5;.5;0.'.  Need to turn into nx3 arr'''
    y = np.array(y)
    y_new = np.zeros((len(y), width))
    for i in range(len(y)):
        if '[0.333' in y[i]:
            y_new[i] = [.333, .333, .333]
            continue
        assert ';' in y[i] or ' ' in y[i]
        char = ';' if ';' in y[i] else None
        y_new[i] = list(map(lambda elt: float(elt), y[i].split(char)))
    return y_new

def get_batch(arr, batch_idx, batch_size):
    return arr[batch_idx * batch_size:(batch_idx + 1) * batch_size]

def sample_batch(x, y, batch_size):
    start = np.random.randint(x.shape[0]-batch_size)
    x_batch = x[start:start+batch_size]
    y_batch = y[start:start+batch_size]
    return x_batch, y_batch

def get_mfbs(paths, lengths_dict, max_dilation_rate):
    max_len = max([lengths_dict[file_name] for file_name in paths])
    max_len = dilation_pad(max_len, max_dilation_rate)
    return paths_to_mfbs(paths, max_len)

def shuffle_data(*arrs):
    rnd_state = np.random.get_state()
    for arr in arrs:
        np.random.shuffle(arr)
        np.random.set_state(rnd_state)

def get_class_weights(arr):
    '''pass in dummies'''
    class_weights = np.nansum(arr, axis=0)
    return np.sum(class_weights) / (class_weights*len(class_weights))

def get_class_weights_ds(arr):
    '''do not pass in dummies'''
    arr = np.stack(np.unique(np.array(arr), return_counts=True), axis=1)
    return (np.sum(arr[:,1]) - arr[:,1]) / arr[:,1]

def trunc(values, decs=0):
    return np.trunc(values*10**decs)/(10**decs)

def path_to_func(path, func_name):
    '''From path, import function from module at that path'''
    import importlib.util
    spec = importlib.util.spec_from_file_location("module.name", path)
    foo = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(foo)
    return getattr(foo, func_name)

    
def batch_split(arr, batch_size):
    num_batches = np.ceil(arr.shape[0] / batch_size)
    return [arr[batch_idx*batch_size:(batch_idx+1)*batch_size] for batch_idx in range(int(num_batches))]



def split_wavs_helper(audio_path):
    sample_rate, audio = wav.read(audio_path)
    id = audio_path.split('/')[-1].split('.')[0]
    segments, sample_rate, audio_length = vad_segment_generator(audio_path, aggressiveness=global_aggressiveness)
    for idx, segment in enumerate(segments):
        audio_segment = np.frombuffer(segment, dtype=np.int16)
        temp_path = join(temp_wav_dir, f'{id}[{idx}].wav')
        wav.write(temp_path, sample_rate, audio_segment)

def split_wavs(wav_dir, temp_wav_dir_in=None, multiproc=True, agg_in=None):
    print('Splitting wavs into VAD chunks for transcription...')
    global orig_wav_dir, temp_wav_dir, p, global_aggressiveness
    if agg_in is not None:
        global_aggressiveness = agg_in
    if temp_wav_dir_in is None:
        temp_wav_dir = join(wav_dir, 'temp')
    else:
        temp_wav_dir = temp_wav_dir_in
    orig_wav_dir = wav_dir

    rmtree(temp_wav_dir)
    audio_paths = glob.glob(os.path.join(wav_dir, '*'))
    rm_mkdirp(temp_wav_dir, overwrite=True, quiet=True)

    if multiproc:
        num_workers = 6
        pool = mp.Pool(num_workers)

        for _ in tqdm(pool.imap_unordered(split_wavs_helper, audio_paths), total=len(audio_paths)):
            pass
        pool.close()
        pool.join()
        
    else:
        for audio_path in tqdm(audio_paths): # if not multiprocessing capable machine or an error
            split_wavs_helper(audio_path)

    return temp_wav_dir